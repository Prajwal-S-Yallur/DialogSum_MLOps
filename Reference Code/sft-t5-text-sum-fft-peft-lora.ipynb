{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6131005,"sourceType":"datasetVersion","datasetId":3515188},{"sourceId":6354265,"sourceType":"datasetVersion","datasetId":3659633},{"sourceId":7448845,"sourceType":"datasetVersion","datasetId":4335839},{"sourceId":136700676,"sourceType":"kernelVersion"},{"sourceId":139165085,"sourceType":"kernelVersion"},{"sourceId":4251,"sourceType":"modelInstanceVersion","modelInstanceId":3045}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tune a Generative AI Model for Dialogue Summarization","metadata":{"tags":[]}},{"cell_type":"markdown","source":"In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics.\n\n- 原链接：https://www.kaggle.com/code/paultimothymooney/fine-tune-flan-t5-with-peft-lora-deeplearning-ai\nSource:\n- https://www.coursera.org/learn/generative-ai-with-llms\n- https://www.coursera.org/learn/generative-ai-with-llms/gradedLti/x0gc1/lab-2-fine-tune-a-generative-ai-model-for-dialogue-summarization\n- https://creativecommons.org/licenses/by-sa/2.0/legalcode","metadata":{}},{"cell_type":"markdown","source":"\n# Table of Contents\n- [ 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM](#1)\n  - [ 1.1 - Set up Kernel and Required Dependencies](#1.1)\n  - [ 1.2 - Load Dataset and LLM](#1.2)\n  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n- [ 2 - Perform Full Fine-Tuning](#2)\n  - [ 2.1 - Preprocess the Dialog-Summary Dataset](#2.1)\n  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n  - [ 3.2 - Train PEFT Adapter](#3.2)\n  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM","metadata":{}},{"cell_type":"markdown","source":"<a name='1.1'></a>\n### 1.1 - Set up Kernel and Required Dependencies","metadata":{}},{"cell_type":"code","source":"# Now install the required packages for the LLM and datasets.\n\n%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    peft==0.3.0 --quiet\n\n!pip install awscli ","metadata":{"tags":[],"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-01-22T02:48:20.900919Z","iopub.execute_input":"2024-01-22T02:48:20.902040Z","iopub.status.idle":"2024-01-22T02:50:55.124508Z","shell.execute_reply.started":"2024-01-22T02:48:20.901987Z","shell.execute_reply":"2024-01-22T02:50:55.123282Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.1.2)\nCollecting pip\n  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.1.2\n    Uninstalling pip-23.1.2:\n      Successfully uninstalled pip-23.1.2\nSuccessfully installed pip-23.3.2\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nCollecting awscli\n  Downloading awscli-1.32.23-py3-none-any.whl.metadata (11 kB)\nCollecting botocore==1.34.23 (from awscli)\n  Downloading botocore-1.34.23-py3-none-any.whl.metadata (5.6 kB)\nCollecting docutils<0.17,>=0.10 (from awscli)\n  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting s3transfer<0.11.0,>=0.10.0 (from awscli)\n  Downloading s3transfer-0.10.0-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: PyYAML<6.1,>=3.10 in /opt/conda/lib/python3.10/site-packages (from awscli) (6.0)\nCollecting colorama<0.4.5,>=0.2.5 (from awscli)\n  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\nCollecting rsa<4.8,>=3.1.2 (from awscli)\n  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore==1.34.23->awscli) (1.0.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore==1.34.23->awscli) (2.8.2)\nRequirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore==1.34.23->awscli) (1.26.15)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.34.23->awscli) (1.16.0)\nDownloading awscli-1.32.23-py3-none-any.whl (4.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading botocore-1.34.23-py3-none-any.whl (11.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: rsa, docutils, colorama, botocore, s3transfer, awscli\n  Attempting uninstall: rsa\n    Found existing installation: rsa 4.9\n    Uninstalling rsa-4.9:\n      Successfully uninstalled rsa-4.9\n  Attempting uninstall: docutils\n    Found existing installation: docutils 0.20.1\n    Uninstalling docutils-0.20.1:\n      Successfully uninstalled docutils-0.20.1\n  Attempting uninstall: colorama\n    Found existing installation: colorama 0.4.6\n    Uninstalling colorama-0.4.6:\n      Successfully uninstalled colorama-0.4.6\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.29.161\n    Uninstalling botocore-1.29.161:\n      Successfully uninstalled botocore-1.29.161\n  Attempting uninstall: s3transfer\n    Found existing installation: s3transfer 0.6.1\n    Uninstalling s3transfer-0.6.1:\n      Successfully uninstalled s3transfer-0.6.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.5.2 requires botocore<1.29.162,>=1.29.161, but you have botocore 1.34.23 which is incompatible.\nbayesian-optimization 1.4.3 requires colorama>=0.4.6, but you have colorama 0.4.4 which is incompatible.\nboto3 1.26.100 requires botocore<1.30.0,>=1.29.100, but you have botocore 1.34.23 which is incompatible.\nboto3 1.26.100 requires s3transfer<0.7.0,>=0.6.0, but you have s3transfer 0.10.0 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed awscli-1.32.23 botocore-1.34.23 colorama-0.4.4 docutils-0.16 rsa-4.7.2 s3transfer-0.10.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport evaluate\nimport pandas as pd\nimport numpy as np","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:50:55.126936Z","iopub.execute_input":"2024-01-22T02:50:55.127332Z","iopub.status.idle":"2024-01-22T02:51:07.037444Z","shell.execute_reply.started":"2024-01-22T02:50:55.127297Z","shell.execute_reply":"2024-01-22T02:51:07.036622Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='1.2'></a>\n### 1.2 - Load Dataset and LLM\n\nYou are going to continue experimenting with the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. ","metadata":{"tags":[]}},{"cell_type":"code","source":"# 加载sft语料\nhuggingface_dataset_name = \"knkarthick/dialogsum\"\ndataset = load_dataset(huggingface_dataset_name)\ndataset\n\n# 打印语料明细\nfor index, row in enumerate(dataset['train']):\n   if index < 2:\n       print(row)\n   else:\n       break","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:51:07.038494Z","iopub.execute_input":"2024-01-22T02:51:07.038751Z","iopub.status.idle":"2024-01-22T02:51:09.822135Z","shell.execute_reply.started":"2024-01-22T02:51:07.038728Z","shell.execute_reply":"2024-01-22T02:51:09.821207Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1afa777017bf4692af0950fdbd29711f"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset csv/knkarthick--dialogsum to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79c44f5ab95342a3b7e0e9565d112396"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a4e0a3f9ae44e52adf5d02ddde17b29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dfd982ee3784dbb89081f1683ec66f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c1ea8171c4460ca4b7434c7a3c95a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"737c761ccb2243a989680b61f9ef9651"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8ff0f0a91b34becb4ee35aca36c3300"}},"metadata":{}},{"name":"stdout","text":"{'id': 'train_0', 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\", 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\", 'topic': 'get a check-up'}\n{'id': 'train_1', 'dialogue': \"#Person1#: Hello Mrs. Parker, how have you been?\\n#Person2#: Hello Dr. Peters. Just fine thank you. Ricky and I are here for his vaccines.\\n#Person1#: Very well. Let's see, according to his vaccination record, Ricky has received his Polio, Tetanus and Hepatitis B shots. He is 14 months old, so he is due for Hepatitis A, Chickenpox and Measles shots.\\n#Person2#: What about Rubella and Mumps?\\n#Person1#: Well, I can only give him these for now, and after a couple of weeks I can administer the rest.\\n#Person2#: OK, great. Doctor, I think I also may need a Tetanus booster. Last time I got it was maybe fifteen years ago!\\n#Person1#: We will check our records and I'll have the nurse administer and the booster as well. Now, please hold Ricky's arm tight, this may sting a little.\", 'summary': 'Mrs Parker takes Ricky for his vaccines. Dr. Peters checks the record and then gives Ricky a vaccine.', 'topic': 'vaccines'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"* #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n\n* #Person2#: I found it would be a good idea to get a check-up.\\n\n* #Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n\n* #Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n\n* #Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n\n* #Person2#: Ok.\\n\n* #Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n* \\n#Person2#: Yes.\\n\n* #Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n\n* #Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n\n* #Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n* \\n#Person2#: Ok, thanks doctor.\",\n*  'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\", \n* 'topic': 'get a check-up'}","metadata":{"execution":{"iopub.status.busy":"2024-01-21T13:25:15.884120Z","iopub.execute_input":"2024-01-21T13:25:15.884847Z","iopub.status.idle":"2024-01-21T13:25:15.890800Z","shell.execute_reply.started":"2024-01-21T13:25:15.884815Z","shell.execute_reply":"2024-01-21T13:25:15.889813Z"}}},{"cell_type":"code","source":"# 加载预训练模型   FLAN-T5 model 及对应的 tokenizer\n# model_name='google/flan-t5-base'\nmodel_name='/kaggle/input/flan-t5/pytorch/base/4'\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16) # 按照模型名称加载模型\ntokenizer = AutoTokenizer.from_pretrained(model_name)  # 按照模型名称加载模型\nprint('Model Loaded')\n\n# 备注：Notice that you will be using the small version of FLAN-T5. Setting torch_dtype=torch.bfloat16 \n#      specifies the memory type to be used by this model.","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:51:09.824964Z","iopub.execute_input":"2024-01-22T02:51:09.825603Z","iopub.status.idle":"2024-01-22T02:51:22.623262Z","shell.execute_reply.started":"2024-01-22T02:51:09.825567Z","shell.execute_reply":"2024-01-22T02:51:22.622284Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Model Loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"# 函数：提取模型参数的数量，并找出其中有多少是可训练的\n\ndef print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()  # 模型的全部参数\n        if param.requires_grad:\n            trainable_model_params += param.numel() # 模型的可训练参数（支持梯度计算=可训练）\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:51:22.624494Z","iopub.execute_input":"2024-01-22T02:51:22.624786Z","iopub.status.idle":"2024-01-22T02:51:22.633389Z","shell.execute_reply.started":"2024-01-22T02:51:22.624760Z","shell.execute_reply":"2024-01-22T02:51:22.632299Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"trainable model parameters: 247577856\nall model parameters: 247577856\npercentage of trainable model parameters: 100.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='1.3'></a>\n### 1.3 - 模型试用(no-sft) Test the Model with Zero Shot Inferencing\n\nTest the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand.","metadata":{"tags":[]}},{"cell_type":"code","source":"# tokenizer 测试\nprompt = f\"\"\"I HAVE A CAR\"\"\"\ninputs = tokenizer(prompt, return_tensors='pt') \ninputs \n\n# 输出：{'input_ids': tensor([[   27, 21490,    71,   205,  4280,     1]]), \n#       'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n# 问题：是每个单词对应着input_ids的1个值么？（经测试不是的）\n\n\n# input_ids: PyTorch张量，代表了prompt文本的Token IDs。\n# Token IDs是模型训练时使用的整数标识符，每个 unique token都有一个对应的ID。\n# 在这里，input_ids是tensor([[27, 43, 3, 9, 443, 3, 1]])，\n# 表示prompt文本被转换为了一个整数序列，其中每个数字代表了词汇表中每个单词的ID。\n\n# attention_mask: PyTorch张量，代表了input_ids中每个token的有效性（即是否应该关注这个token）。\n# 在这里，attention_mask是tensor([[1, 1, 1, 1, 1, 1, 1]])，这意味着所有输入的token都应该被模型关注。\n\n# attention_mask用于标识实际的Token和填充Token的掩码。在处理文本序列时，由于序列的长度可能不同，\n# 因此需要使用填充Token来对齐不同长度的序列。attention_mask的作用是告诉模型哪些位置上的Token是实际的Token，\n# 哪些位置上的Token是填充的Token。这样，模型在计算注意力权重时可以避免对填充的Token给予过多的关注。\n\n# Token IDs 和 attention_mask 是同维度的","metadata":{"execution":{"iopub.status.busy":"2024-01-22T02:51:22.634657Z","iopub.execute_input":"2024-01-22T02:51:22.634981Z","iopub.status.idle":"2024-01-22T02:51:22.701016Z","shell.execute_reply.started":"2024-01-22T02:51:22.634949Z","shell.execute_reply":"2024-01-22T02:51:22.699996Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[   27, 21490,    71,   205,  4280,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"code","source":"index = 200\n\ndialogue = dataset['test'][index]['dialogue']  # 待summary文本\nsummary = dataset['test'][index]['summary']    # summary\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninputs = tokenizer(prompt, return_tensors='pt') # return_tensors='pt' ：将输出转换为tensors\n# 将prompt文本转换为模型可以接受的格式\n\noutput = tokenizer.decode(\n    original_model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=200,\n    )[0], \n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:51:22.702348Z","iopub.execute_input":"2024-01-22T02:51:22.702686Z","iopub.status.idle":"2024-01-22T02:51:24.296967Z","shell.execute_reply.started":"2024-01-22T02:51:22.702658Z","shell.execute_reply":"2024-01-22T02:51:24.295935Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nSummary:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n#Person1#: I'm thinking of upgrading my computer.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='2'></a>\n## 2 - Perform Full Fine-Tuning （全参数微调）","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='2.1'></a>\n### 2.1 - Preprocess the Dialog-Summary Dataset\n\nYou need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation` and to the start of the summary with `Summary` as follows:\n\nTraining prompt (dialogue):\n```\nSummarize the following conversation.\n\n    Chris: This is his part of the conversation.\n    Antje: This is her part of the conversation.\n    \nSummary: \n```\n\nTraining response (summary):\n```\nBoth Chris and Antje participated in the conversation.\n```\n\nThen preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token).","metadata":{"tags":[]}},{"cell_type":"code","source":"# 数据预处理\n# 构建“prompt-response”的pair 的input_id\ndef tokenize_function(example):\n    start_prompt = 'Summarize the following conversation.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    \n    return example\n\n# The dataset actually contains 3 diff splits: train, validation, test.\n# The tokenize_function code is handling all data across all splits in batches.\n\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])\n\nprint(dataset.shape)\n# {'train': (12460, 4), 'test': (1500, 4), 'validation': (500, 4)}\nprint(tokenized_datasets.shape)\n#{'train': (12460, 2), 'test': (1500, 2), 'validation': (500, 2)}","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T04:02:11.400674Z","iopub.execute_input":"2024-01-22T04:02:11.401342Z","iopub.status.idle":"2024-01-22T04:02:11.444088Z","shell.execute_reply.started":"2024-01-22T04:02:11.401307Z","shell.execute_reply":"2024-01-22T04:02:11.443067Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"{'train': (12460, 4), 'test': (1500, 4), 'validation': (500, 4)}\n{'train': (12460, 2), 'test': (1500, 2), 'validation': (500, 2)}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To save some time in the lab, you will subsample the dataset:","metadata":{"tags":[]}},{"cell_type":"code","source":"# 数据抽样\n# To save some time in the lab, you will subsample the dataset:\n\nprint('采样前：',tokenized_datasets.shape)\ntokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 1 == 0, with_indices=True)\n                    # 保留tokenized_datasets数据集中的每100个元素的子集\n\n\n# 查看抽样后，train、test、valid各自的数据量\n# Check the shapes of all three parts of the dataset:\nprint('采样后：')\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {tokenized_datasets['train'].shape}\")\nprint(f\"Validation: {tokenized_datasets['validation'].shape}\")\nprint(f\"Test: {tokenized_datasets['test'].shape}\")\n\nprint(tokenized_datasets)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T04:02:14.273970Z","iopub.execute_input":"2024-01-22T04:02:14.274709Z","iopub.status.idle":"2024-01-22T04:02:23.754418Z","shell.execute_reply.started":"2024-01-22T04:02:14.274677Z","shell.execute_reply":"2024-01-22T04:02:23.753477Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"采样前： {'train': (12460, 2), 'test': (1500, 2), 'validation': (500, 2)}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"采样后：\nShapes of the datasets:\nTraining: (12460, 2)\nValidation: (500, 2)\nTest: (1500, 2)\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 12460\n    })\n    test: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 1500\n    })\n    validation: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 500\n    })\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='2.2'></a>\n### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n\nNow utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment.","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport time\nfrom transformers import TrainingArguments, Trainer\n \n# 设置环境变量以禁用 Weights & Biases 跟踪\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n \n# 创建一个输出目录，该目录包含当前时间戳，以便每个训练任务都有独特的存储位置\noutput_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n \n# 定义训练参数\ntraining_args = TrainingArguments(\n   output_dir=output_dir,  # 输出目录\n   learning_rate=1e-5,      # 学习率\n   num_train_epochs=1,      # 训练轮数\n   weight_decay=0.01,       # 权重衰减\n   logging_steps=1,         # 日志记录步数\n   max_steps=1,             # 最大训练步数\n   report_to=None           # 不报告训练进度\n)\n \n# 初始化 Trainer 类\ntrainer = Trainer(\n   model=original_model,  # 用于微调的预训练模型\n   args=training_args,    # 训练参数\n   train_dataset=tokenized_datasets['train'],  # 训练数据集\n   eval_dataset=tokenized_datasets['validation']  # 验证数据集\n)\n \n# 注意：以上代码假设您已经定义了 'original_model' 和 'tokenized_datasets'。\n# 'tokenized_datasets' 是一个包含已经分词和编码的数据集的字典。","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:51:44.400378Z","iopub.execute_input":"2024-01-22T02:51:44.400692Z","iopub.status.idle":"2024-01-22T02:51:45.525877Z","shell.execute_reply.started":"2024-01-22T02:51:44.400665Z","shell.execute_reply":"2024-01-22T02:51:45.525069Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Start training process...\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5Zb3UgY2FuIHNhZmVseSBpZ25vcmUgdGhlIHdhcm5pbmcgbWVzc2FnZXMuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>","metadata":{"tags":[]}},{"cell_type":"code","source":"# 训练...启动！！！\n# 记得切换GPU哦\ntrainer.train()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:51:45.527022Z","iopub.execute_input":"2024-01-22T02:51:45.527319Z","iopub.status.idle":"2024-01-22T02:51:47.246735Z","shell.execute_reply.started":"2024-01-22T02:51:45.527293Z","shell.execute_reply":"2024-01-22T02:51:47.245931Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:00, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>48.500000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1, training_loss=48.5, metrics={'train_runtime': 1.6936, 'train_samples_per_second': 4.724, 'train_steps_per_second': 0.59, 'total_flos': 5478058819584.0, 'train_loss': 48.5, 'epoch': 0.06})"},"metadata":{}}]},{"cell_type":"markdown","source":"* Training a fully fine-tuned version of the model would take a few hours on a GPU. To save time, download a checkpoint of the fully fine-tuned model to use in the rest of this notebook. This fully fine-tuned model will also be referred to as the **instruct model** in this lab.\n* 在GPU上训练一个完全微调的模型版本需要几个小时。为了节省时间，请下载一个完全微调模型的检查点，用于此笔记本的其余部分。这个完全微调的模型也将被称为本实验室的指导模型。","metadata":{}},{"cell_type":"code","source":"!aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:51:47.247864Z","iopub.execute_input":"2024-01-22T02:51:47.248148Z","iopub.status.idle":"2024-01-22T02:51:54.918005Z","shell.execute_reply.started":"2024-01-22T02:51:47.248123Z","shell.execute_reply":"2024-01-22T02:51:54.916709Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nfatal error: Unable to locate credentials\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The size of the downloaded instruct model is approximately 1GB.","metadata":{"tags":[]}},{"cell_type":"code","source":"!ls -alh /kaggle/working/flan-dialogue-summary-checkpoint/pytorch_model.bin","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:51:54.919780Z","iopub.execute_input":"2024-01-22T02:51:54.920195Z","iopub.status.idle":"2024-01-22T02:51:55.898024Z","shell.execute_reply.started":"2024-01-22T02:51:54.920164Z","shell.execute_reply":"2024-01-22T02:51:55.896828Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nls: cannot access '/kaggle/working/flan-dialogue-summary-checkpoint/pytorch_model.bin': No such file or directory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:","metadata":{"tags":[]}},{"cell_type":"code","source":"# 定义“初始模型”\noriginal_model = original_model.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T02:51:55.899862Z","iopub.execute_input":"2024-01-22T02:51:55.900272Z","iopub.status.idle":"2024-01-22T02:51:56.722560Z","shell.execute_reply.started":"2024-01-22T02:51:55.900220Z","shell.execute_reply":"2024-01-22T02:51:56.721538Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# # 定义“全参微调后的模型”\n# instruct_model ： fully fine-tuned model \n#instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/generative-ai-with-llms-lab-2/lab_2/flan-dialogue-summary-checkpoint/\", \n#                                                       torch_dtype=torch.bfloat16).to('cpu')\n\n# 以上代码有报错，无法载入“全量微调后的模型”，暂时先用微调前模型替代\ninstruct_model = original_model","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:51:56.723735Z","iopub.execute_input":"2024-01-22T02:51:56.724029Z","iopub.status.idle":"2024-01-22T02:51:56.728656Z","shell.execute_reply.started":"2024-01-22T02:51:56.724004Z","shell.execute_reply":"2024-01-22T02:51:56.727722Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<a name='2.3'></a>\n### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)（定性评价）\n### 查看全量微调后的模型效果（主观评价）\n\nAs with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model.","metadata":{}},{"cell_type":"code","source":"index = 200  # 在测试集中随机抽取1条数据\ndialogue = dataset['test'][index]['dialogue']\nhuman_baseline_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n# input输入model，model推理output\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n# 将推理output解码为自然语言\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:51:56.729914Z","iopub.execute_input":"2024-01-22T02:51:56.730196Z","iopub.status.idle":"2024-01-22T02:52:06.931706Z","shell.execute_reply.started":"2024-01-22T02:51:56.730171Z","shell.execute_reply":"2024-01-22T02:52:06.930553Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nORIGINAL MODEL:\n#Person1: Have you considered upgrading your system? #Person2: Yes, I'm not sure what exactly I would need. #Person1: You could consider adding a painting program to your software. #Person2: You might want to add a CD-ROM drive. #Person1: No, I'm not sure. #Person2: I'm not sure what I would need. #Person2: I'd probably need a more powerful processor, to begin with. #Person1: You might want to upgrade your hardware because it's pretty outdated now. #Person2: You could also consider adding a CD-ROM drive. #Person2: I'm not sure what I would need.\n---------------------------------------------------------------------------------------------------\nINSTRUCT MODEL:\n#Person1#: Have you considered upgrading your computer?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='2.4'></a>\n### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)（定量评价）\n\nThe [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:52:06.933330Z","iopub.execute_input":"2024-01-22T02:52:06.933713Z","iopub.status.idle":"2024-01-22T02:52:07.987065Z","shell.execute_reply.started":"2024-01-22T02:52:06.933678Z","shell.execute_reply":"2024-01-22T02:52:07.986294Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"908f6d934c1541fc85dbb4ffefbceb74"}},"metadata":{}}]},{"cell_type":"markdown","source":"Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results.","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']  # 为了节省时间，从测试集抽取10条样本用于评价\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\n\nfor _, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n    original_model_summaries.append(original_model_text_output)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n    instruct_model_summaries.append(instruct_model_text_output)\n    \nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\ndf","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:52:07.988397Z","iopub.execute_input":"2024-01-22T02:52:07.988752Z","iopub.status.idle":"2024-01-22T02:53:27.267733Z","shell.execute_reply.started":"2024-01-22T02:52:07.988719Z","shell.execute_reply":"2024-01-22T02:53:27.266816Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n7  #Person1# tells Kate that Masha and Hero are g...   \n8  #Person1# and Kate talk about the divorce betw...   \n9  #Person1# and Brian are at the birthday party ...   \n\n                            original_model_summaries  \\\n0  #Person1#: Ms. Dawson, this memo is for the af...   \n1                    This memo is for all employees.   \n2  #Person1: Please type in a memo to all employe...   \n3  #Person1: I'm here! #Person2: I'm finally here...   \n4  #Person1#: You're finally here! #Person2#: I'm...   \n5  #Person1: You're here! #Person2: I'm glad you'...   \n6               Masha and Hero are getting divorced.   \n7               Masha and Hero are getting divorced.   \n8                       Masha and Hero are divorced.   \n9  Brian's birthday party is going to be a great ...   \n\n                            instruct_model_summaries  \n0  #Person2#: I need to take a dictation for you....  \n1  The Office of the President and CEO has issued...  \n2   Employees are required to use instant messaging.  \n3         The driver of the car is stuck in traffic.  \n4  The Carrefour office is closed at the end of t...  \n5  #Person1: I'm a traffic expert. #Person2: I'm ...  \n6     #Person1: Masha and Hero are getting divorced.  \n7  #Person1#: Masha and Hero are getting divorced...  \n8  #Person1: Masha and Hero are getting divorced....  \n9           People are celebrating Brian's birthday.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>instruct_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>#Person1#: Ms. Dawson, this memo is for the af...</td>\n      <td>#Person2#: I need to take a dictation for you....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>This memo is for all employees.</td>\n      <td>The Office of the President and CEO has issued...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>#Person1: Please type in a memo to all employe...</td>\n      <td>Employees are required to use instant messaging.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>#Person1: I'm here! #Person2: I'm finally here...</td>\n      <td>The driver of the car is stuck in traffic.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>#Person1#: You're finally here! #Person2#: I'm...</td>\n      <td>The Carrefour office is closed at the end of t...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>#Person1: You're here! #Person2: I'm glad you'...</td>\n      <td>#Person1: I'm a traffic expert. #Person2: I'm ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>#Person1: Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>#Person1#: Masha and Hero are getting divorced...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>Masha and Hero are divorced.</td>\n      <td>#Person1: Masha and Hero are getting divorced....</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>Brian's birthday party is going to be a great ...</td>\n      <td>People are celebrating Brian's birthday.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Evaluate the models computing ROUGE metrics. Notice the improvement in the results!","metadata":{"tags":[]}},{"cell_type":"code","source":"# 假设 rouge 是一个已经导入的 Rouge 评估库的实例，用于评估自动生成的摘要的质量和准确性。\n# original_model_summaries 是使用原始模型生成的摘要列表。\n# human_baseline_summaries 是一个包含人类生成的基准摘要列表。\n\n# 微调前模型\noriginal_model_results = rouge.compute(\n   predictions=original_model_summaries,  # 预测的摘要列表\n   references=human_baseline_summaries[0:len(original_model_summaries)],  # 基准摘要列表，长度与预测摘要相同\n   use_aggregator=True,  # 使用聚合器来计算平均分数\n   use_stemmer=True  # 使用词干提取器来简化单词\n)\n \n# # 微调后模型\n# instruct_model_summaries 是使用指导模型（可能是在特定指令下训练的模型）生成的摘要列表。\ninstruct_model_results = rouge.compute(\n   predictions=instruct_model_summaries,  # 预测的摘要列表\n   references=human_baseline_summaries[0:len(instruct_model_summaries)],  # 基准摘要列表，长度与预测摘要相同\n   use_aggregator=True,  # 使用聚合器来计算平均分数\n   use_stemmer=True  # 使用词干提取器来简化单词\n)\n \n# 打印原始模型和指导模型的评估结果\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:53:27.268974Z","iopub.execute_input":"2024-01-22T02:53:27.269295Z","iopub.status.idle":"2024-01-22T02:53:27.755658Z","shell.execute_reply.started":"2024-01-22T02:53:27.269265Z","shell.execute_reply":"2024-01-22T02:53:27.754643Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2625896510948609, 'rouge2': 0.09789459936773273, 'rougeL': 0.2202812192302071, 'rougeLsum': 0.22361512426226335}\nINSTRUCT MODEL:\n{'rouge1': 0.23379888309748356, 'rouge2': 0.06717851592851594, 'rougeL': 0.20473865264656105, 'rougeLsum': 0.20677207847136939}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- The file `data/dialogue-summary-training-results.csv` contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models:\n- 文件data/dialogue-summary-training-results.csv包含一个预先填充的所有模型结果列表，您可以使用该列表对更大的数据部分进行评估。让我们为每个模型执行以下操作：","metadata":{}},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2024-01-22T03:13:10.035544Z","iopub.execute_input":"2024-01-22T03:13:10.036538Z","iopub.status.idle":"2024-01-22T03:13:10.050980Z","shell.execute_reply.started":"2024-01-22T03:13:10.036496Z","shell.execute_reply":"2024-01-22T03:13:10.049978Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"      Unnamed: 0                           human_baseline_summaries  \\\n0              0  Ms. Dawson helps #Person1# to write a memo to ...   \n1              1  In order to prevent employees from wasting tim...   \n2              2  Ms. Dawson takes a dictation for #Person1# abo...   \n3              3  #Person2# arrives late because of traffic jam....   \n4              4  #Person2# decides to follow #Person1#'s sugges...   \n...          ...                                                ...   \n1495        1495  Matthew and Steve meet after a long time. Stev...   \n1496        1496  Steve has been looking for a place to live. Ma...   \n1497        1497  Frank invites Besty to the party to celebrate ...   \n1498        1498  Frank invites Betsy to the big promotion party...   \n1499        1499  Frank invites Betsy to his party for his promo...   \n\n                               original_model_summaries  \\\n0     The memo is to be distributed to all employees...   \n1     The memo is to be distributed to all employees...   \n2     The memo is to be distributed to all employees...   \n3     The traffic jam at the Carrefour intersection ...   \n4     The traffic jam at the Carrefour intersection ...   \n...                                                 ...   \n1495  #Person1#: Hi! #Person2#: Hi! How are you? #Pe...   \n1496  #Person1#: Hi! #Person2#: Hi! How are you? #Pe...   \n1497  Person1 is going to throw a party for all of h...   \n1498  Person1 is going to throw a party for all of h...   \n1499  Person1 is going to throw a party for all of h...   \n\n                               instruct_model_summaries  \\\n0     #Person1# asks Ms. Dawson to take a dictation ...   \n1     #Person1# asks Ms. Dawson to take a dictation ...   \n2     #Person1# asks Ms. Dawson to take a dictation ...   \n3     #Person2# got stuck in traffic again. #Person1...   \n4     #Person2# got stuck in traffic again. #Person1...   \n...                                                 ...   \n1495  Steve hasn't seen Matthew for a year. He's bee...   \n1496  Steve hasn't seen Matthew for a year. He's bee...   \n1497  Frank invites Betsy to his promotion party on ...   \n1498  Frank invites Betsy to his promotion party on ...   \n1499  Frank invites Betsy to his promotion party on ...   \n\n                                   peft_model_summaries  \n0     #Person1# asks Ms. Dawson to take a dictation ...  \n1     #Person1# asks Ms. Dawson to take a dictation ...  \n2     #Person1# asks Ms. Dawson to take a dictation ...  \n3     #Person2# got stuck in traffic and got stuck i...  \n4     #Person2# got stuck in traffic and got stuck i...  \n...                                                 ...  \n1495  Matthew and Steve are looking for a place to l...  \n1496  Matthew and Steve are looking for a place to l...  \n1497  Frank invites Betsy to a party for all of his ...  \n1498  Frank invites Betsy to a party for all of his ...  \n1499  Frank invites Betsy to a party for all of his ...  \n\n[1500 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>instruct_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>The memo is to be distributed to all employees...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>The memo is to be distributed to all employees...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>The memo is to be distributed to all employees...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n      <td>#Person2# got stuck in traffic again. #Person1...</td>\n      <td>#Person2# got stuck in traffic and got stuck i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n      <td>#Person2# got stuck in traffic again. #Person1...</td>\n      <td>#Person2# got stuck in traffic and got stuck i...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>1495</td>\n      <td>Matthew and Steve meet after a long time. Stev...</td>\n      <td>#Person1#: Hi! #Person2#: Hi! How are you? #Pe...</td>\n      <td>Steve hasn't seen Matthew for a year. He's bee...</td>\n      <td>Matthew and Steve are looking for a place to l...</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>1496</td>\n      <td>Steve has been looking for a place to live. Ma...</td>\n      <td>#Person1#: Hi! #Person2#: Hi! How are you? #Pe...</td>\n      <td>Steve hasn't seen Matthew for a year. He's bee...</td>\n      <td>Matthew and Steve are looking for a place to l...</td>\n    </tr>\n    <tr>\n      <th>1497</th>\n      <td>1497</td>\n      <td>Frank invites Besty to the party to celebrate ...</td>\n      <td>Person1 is going to throw a party for all of h...</td>\n      <td>Frank invites Betsy to his promotion party on ...</td>\n      <td>Frank invites Betsy to a party for all of his ...</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>1498</td>\n      <td>Frank invites Betsy to the big promotion party...</td>\n      <td>Person1 is going to throw a party for all of h...</td>\n      <td>Frank invites Betsy to his promotion party on ...</td>\n      <td>Frank invites Betsy to a party for all of his ...</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>1499</td>\n      <td>Frank invites Betsy to his party for his promo...</td>\n      <td>Person1 is going to throw a party for all of h...</td>\n      <td>Frank invites Betsy to his promotion party on ...</td>\n      <td>Frank invites Betsy to a party for all of his ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1500 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 评价全量预测结果的评估分数\n# 文件结果来自全量微调后的模型的推理\nresults = pd.read_csv(\"/kaggle/input/dialogue-summary-training-results/dialogue-summary-training-results.csv\")\n#results = pd.read_csv(\"/kaggle/input/generative-ai-with-llms-lab-2/lab_2/data/dialogue-summary-training-results.csv\")\n# df里包含index、人工的总结、微调前模型的总结、全参微调后模型的总结、参数微调后模型的总结\n# human_baseline_summaries\toriginal_model_summaries\tinstruct_model_summaries\tpeft_model_summaries\n\n\nhuman_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\n\n# 指标评估 ROUGE分数\n# ROUGE是一种用于评估文本摘要（或其他自然语言处理任务）质量的指标。\n# 与BLEU不同，ROUGE主要关注机器生成的摘要中是否捕捉到了参考摘要的信息，着重于涵盖参考摘要的内容和信息的完整性。\n# ROUGE通过计算N-gram的共现情况，来评估机器生成的摘要的召回率（Recall）","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:53:27.757194Z","iopub.execute_input":"2024-01-22T02:53:27.757626Z","iopub.status.idle":"2024-01-22T02:53:36.996671Z","shell.execute_reply.started":"2024-01-22T02:53:27.757588Z","shell.execute_reply":"2024-01-22T02:53:36.995656Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\nINSTRUCT MODEL:\n{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Rouge 分数是评估自动生成的文本摘要质量的一种常用指标，它主要包括以下几种评分方式：\n* Rouge-1: 最常用，测量的是精确匹配的长度为 1 的单词或短语的数量。简单来说，它计算的是模型生成的摘要中与基准摘要完全相同的单字或短语的数量。\n* Rouge-2: 测量的是长度为 2 的完全匹配的数量，以及长度为 1 的单词或短语的精确匹配的数量。这有助于评估模型摘要中是否有更长的短语与基准摘要相匹配。\n* Rouge-L: Rouge-1和Rouge-2的加权组合，使用长度为1和2的单词或短语的匹配计算分数。Rouge-L更全面，因为考虑了不同长度的匹配，但它仍侧重于较短的匹配(1和2长度)\n* Rouge-Lsum: 改进的 Rouge-L 分数，考虑所有单词的精准匹配，不仅仅是长度为1和2的短语。这意味着 Rouge-Lsum 会给出更全面的摘要质量评估，因为它不限制匹配的长度。\n* 总的来说，Rouge 分数越高，表示自动生成的摘要与人类生成的摘要越相似，质量越高。不同的 Rouge 分数类型关注不同的细节，但它们都是评估摘要质量的有用工具。在实际应用中，通常会报告这些分数的综合表现，以便全面了解模型的性能。\n\n#### The results show substantial improvement in all ROUGE metrics:\n#### 结果来看，全参微调模型的表现明显优于微调前模型","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\nprint(\"INSTRUCT MODEL相对于人类基线的绝对百分比改进\")\n\nimprovement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(instruct_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T03:19:56.090832Z","iopub.execute_input":"2024-01-22T03:19:56.091280Z","iopub.status.idle":"2024-01-22T03:19:56.098296Z","shell.execute_reply.started":"2024-01-22T03:19:56.091243Z","shell.execute_reply":"2024-01-22T03:19:56.097313Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\nINSTRUCT MODEL相对于人类基线的绝对百分比改进\nrouge1: 18.82%\nrouge2: 10.43%\nrougeL: 13.70%\nrougeLsum: 13.69%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='3'></a>\n## 3 - Perform Parameter Efficient Fine-Tuning (PEFT)\n\nNow, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon. \n\nPEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n\nThat said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases.","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning\n\nYou need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n \n# 导入LoRA配置类和获取LoRA模型的函数\n# 以及任务类型的枚举，这里用于指定序列到序列的语言模型任务\n \n# 第1步：创建LoRA配置对象\nlora_config = LoraConfig(\n   r=32,  # LoRA的秩，即低秩分解的秩，这里设置为32，hyper-parameter, which defines the rank/dimension of the adapter to be trained.\n   lora_alpha=32,  # LoRA的超参数alpha，控制正则化的强度，这里设置为32\n   target_modules=[\"q\", \"v\"],  # 指定要进行LoRA适应的模型模块，这里是指定查询(q)和值(v)模块\n   lora_dropout=0.05,  # LoRAdropout率，这里设置为0.05\n   bias=\"none\",  # 指定是否在LoRA适配层中使用偏置，这里设置为\"none\"表示不使用偏置\n   task_type=TaskType.SEQ_2_SEQ_LM  # 指定任务类型为序列到序列的语言模型(FLAN-T5)\n)\n \n# 注意：FLAN-T5是一个特定的模型架构，它使用了FLAN（Fine-tuning with LoRA Adaptation Network）方法\n# 以及T5（Text-to-Text Transfer Transformer）模型的架构。这里的TaskType.SEQ_2_SEQ_LM是指这种类型的任务。\n# 一旦配置好LoRA参数，就可以使用get_peft_model函数根据这些参数获取LoRA适配的模型","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T04:07:21.176895Z","iopub.execute_input":"2024-01-22T04:07:21.177737Z","iopub.status.idle":"2024-01-22T04:07:21.184068Z","shell.execute_reply.started":"2024-01-22T04:07:21.177687Z","shell.execute_reply":"2024-01-22T04:07:21.183198Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"Add LoRA adapter layers/parameters to the original LLM to be trained.","metadata":{"tags":[]}},{"cell_type":"code","source":"# 第2步：加载待sft的模型\n# Add LoRA adapter layers/parameters to the original LLM to be trained.\n# 在模型上面加一个LoRA adapter层\npeft_model = get_peft_model(original_model,   # 微调前模型\n                            lora_config)      # sft参数\nprint(print_number_of_trainable_model_parameters(peft_model)) \n# 支持训练的参数其实就是新增的lora层的参数","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T04:07:22.580045Z","iopub.execute_input":"2024-01-22T04:07:22.580752Z","iopub.status.idle":"2024-01-22T04:07:22.681054Z","shell.execute_reply.started":"2024-01-22T04:07:22.580717Z","shell.execute_reply":"2024-01-22T04:07:22.680117Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"trainable model parameters: 3538944\nall model parameters: 251116800\npercentage of trainable model parameters: 1.41%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='3.2'></a>\n### 3.2 - Train PEFT Adapter\n\nDefine training arguments and create `Trainer` instance.","metadata":{"tags":[]}},{"cell_type":"code","source":"output_dir = f'/kaggle/working/peft-dialogue-summary-training-{str(int(time.time()))}'\n \n# 设置PEFT训练参数\npeft_training_args = TrainingArguments(\n  output_dir=output_dir,  # 设置输出目录为当前时间戳指定的路径\n  auto_find_batch_size=True,  # 自动找到合适的批量大小\n  learning_rate=0.001,                      # 设置学习率为1e-3（0.001），这比完全微调时的学习率要高\n  num_train_epochs=10,  # 设置训练轮数为1\n  logging_steps=1,  # 设置日志记录步数为1\n  max_steps=1  # 设置最大训练步数为1\n)\n \npeft_trainer = Trainer(\n  model=peft_model,  # 使用PEFT适配后的模型\n  args=peft_training_args,  # 训练参数\n  train_dataset=tokenized_datasets[\"train\"],  # 训练数据集\n)\n \ntokenized_datasets[\"train\"].shape\n    \n# 注意：代码中缺少了eval_dataset参数，如果需要评估，应该添加eval_dataset参数\n# 例如：eval_dataset=tokenized_datasets[\"validation\"]\n \n# 一旦配置好训练参数，就可以使用Trainer类来执行训练过程","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T04:08:39.049137Z","iopub.execute_input":"2024-01-22T04:08:39.049639Z","iopub.status.idle":"2024-01-22T04:08:39.075563Z","shell.execute_reply.started":"2024-01-22T04:08:39.049601Z","shell.execute_reply":"2024-01-22T04:08:39.074608Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"(12460, 2)"},"metadata":{}}]},{"cell_type":"markdown","source":"Now everything is ready to train the PEFT adapter and save the model.\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>","metadata":{}},{"cell_type":"code","source":"import time\n \n# start time\nstart_time = time.time()\n \n    \n# 训练... 启动!\npeft_trainer.train()\n\npeft_model_path=\"/kaggle/working/peft-dialogue-summary-checkpoint-local\"\n\npeft_trainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)\n\n\n# end time\nend_time = time.time()\ntraining_duration = end_time - start_time\nprint(f\"Training duration: {training_duration} seconds\")\n\n# 0.001   Training Loss=48.5 1轮\n# 0.0001  Training Loss=48.5","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T04:08:40.872997Z","iopub.execute_input":"2024-01-22T04:08:40.873708Z","iopub.status.idle":"2024-01-22T04:08:43.241772Z","shell.execute_reply.started":"2024-01-22T04:08:40.873669Z","shell.execute_reply":"2024-01-22T04:08:43.240811Z"},"trusted":true},"execution_count":104,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:00, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>33.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training duration: 2.362297773361206 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"That training was performed on a subset of data. To load a fully trained PEFT model, read a checkpoint of a PEFT model from S3.","metadata":{"tags":[]}},{"cell_type":"code","source":"!aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ /kaggle/working/peft-dialogue-summary-checkpoint-from-s3/ ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:53:40.284060Z","iopub.execute_input":"2024-01-22T02:53:40.284469Z","iopub.status.idle":"2024-01-22T02:53:48.000480Z","shell.execute_reply.started":"2024-01-22T02:53:40.284435Z","shell.execute_reply":"2024-01-22T02:53:47.999404Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nfatal error: Unable to locate credentials\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Check that the size of this model is much less than the original LLM:","metadata":{"tags":[]}},{"cell_type":"code","source":"!ls -al /kaggle/working/peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:53:48.001995Z","iopub.execute_input":"2024-01-22T02:53:48.002372Z","iopub.status.idle":"2024-01-22T02:53:48.999527Z","shell.execute_reply.started":"2024-01-22T02:53:48.002341Z","shell.execute_reply":"2024-01-22T02:53:48.998337Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nls: cannot access '/kaggle/working/peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin': No such file or directory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`.","metadata":{"tags":[]}},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\npeft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/flan-t5/pytorch/base/4\", torch_dtype=torch.bfloat16)\n\n\n\n#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/flan-t5/pytorch/base/4\")\n\n#peft_model = PeftModel.from_pretrained(peft_model_base, \n#                                       '/kaggle/input/generative-ai-with-llms-lab-2/lab_2/peft-dialogue-summary-checkpoint-from-s3', \n#                                       torch_dtype=torch.bfloat16,\n#                                      is_trainable=False\n#                                      )\npeft_model = peft_model_base\n# 因报错修改","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:53:49.006569Z","iopub.execute_input":"2024-01-22T02:53:49.006911Z","iopub.status.idle":"2024-01-22T02:53:54.021189Z","shell.execute_reply.started":"2024-01-22T02:53:49.006881Z","shell.execute_reply":"2024-01-22T02:53:54.020397Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"The number of trainable parameters will be `0` due to `is_trainable=False` setting:","metadata":{"tags":[]}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:53:54.022395Z","iopub.execute_input":"2024-01-22T02:53:54.022685Z","iopub.status.idle":"2024-01-22T02:53:54.029927Z","shell.execute_reply.started":"2024-01-22T02:53:54.022658Z","shell.execute_reply":"2024-01-22T02:53:54.028996Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"trainable model parameters: 247577856\nall model parameters: 247577856\npercentage of trainable model parameters: 100.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='3.3'></a>\n### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nMake inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model.","metadata":{}},{"cell_type":"code","source":"peft_model = peft_model.to('cpu')\ninstruct_model = instruct_model.to('cpu')\noriginal_model = original_model.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T02:53:54.031253Z","iopub.execute_input":"2024-01-22T02:53:54.031554Z","iopub.status.idle":"2024-01-22T02:53:54.934799Z","shell.execute_reply.started":"2024-01-22T02:53:54.031529Z","shell.execute_reply":"2024-01-22T02:53:54.933967Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"index = 200\ndialogue = dataset['test'][index]['dialogue']\nbaseline_human_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\npeft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\npeft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\nprint(dash_line)\nprint(f'PEFT MODEL: {peft_model_text_output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:53:54.936020Z","iopub.execute_input":"2024-01-22T02:53:54.936419Z","iopub.status.idle":"2024-01-22T02:54:12.134139Z","shell.execute_reply.started":"2024-01-22T02:53:54.936382Z","shell.execute_reply":"2024-01-22T02:54:12.133117Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nORIGINAL MODEL:\n#Person1#: I'm not sure what I'm talking about. #Person2#: I'm not sure what I'm talking about. #Person1#: I'm not sure what I'm talking about. #Person2#: I'm not sure what I'm talking about. #Person1#: I'm not sure what I'm talking about. #Person2#: I'm not sure what I'm talking about. #Person1#: I'm not sure what I'm talking about. #Person2#: I'm not sure what I'm talking about. #Person1#: I'm not sure what I'm talking about. #Person1#: I'm not sure what I'm talking about.\n---------------------------------------------------------------------------------------------------\nINSTRUCT MODEL:\n#Person1#: Have you considered upgrading your computer? #Person1#: Yes, but I'm not sure what exactly you would need. #Person1#: How about adding a painting program to your computer? #Person1#: I'd like to add a CD-ROM drive.\n---------------------------------------------------------------------------------------------------\nPEFT MODEL: #Person1#: I'm thinking of upgrading my computer.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='3.4'></a>\n### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). ","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    human_baseline_text_output = human_baseline_summaries[idx]\n    \n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\n    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\n    original_model_summaries.append(original_model_text_output)\n    instruct_model_summaries.append(instruct_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:54:12.135869Z","iopub.execute_input":"2024-01-22T02:54:12.136583Z","iopub.status.idle":"2024-01-22T02:55:29.752064Z","shell.execute_reply.started":"2024-01-22T02:54:12.136547Z","shell.execute_reply":"2024-01-22T02:55:29.751277Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n7  #Person1# tells Kate that Masha and Hero are g...   \n8  #Person1# and Kate talk about the divorce betw...   \n9  #Person1# and Brian are at the birthday party ...   \n\n                            original_model_summaries  \\\n0  Ms. Dawson will be out for an intra-office mem...   \n1  Employees will be required to use instant mess...   \n2  The following memo is to be sent out to all em...   \n3                             #Person1: You're here!   \n4  People are talking about the traffic jams in t...   \n5  #P Person1#: I'm sorry to hear about your car ...   \n6                       Masha and Hero are divorced.   \n7               Masha and Hero are getting divorced.   \n8               Masha and Hero are getting divorced.   \n9  #Person1#: Happy Birthday, Brian. #Person2#: Y...   \n\n                            instruct_model_summaries  \\\n0                   This memo is for employees only.   \n1  #Person1#: Ms. Dawson, please type this memo i...   \n2                      This is an intra-office memo.   \n3  The conversation is about the future of public...   \n4  #Person1: I'm going to the office to work. #Pe...   \n5  Taking public transport to work is a good option.   \n6               Masha and Hero are getting divorced.   \n7           People are talking about Masha and Hero.   \n8               Masha and Hero are getting divorced.   \n9                  #Person1#: Happy Birthday, Brian.   \n\n                                peft_model_summaries  \n0     #Person1#: I need to take a dictation for you.  \n1     #Person1#: I need to take a dictation for you.  \n2     #Person1#: I need to take a dictation for you.  \n3  The traffic jam at the Carrefour intersection ...  \n4  The traffic jam at the Carrefour intersection ...  \n5  The traffic jam at the Carrefour intersection ...  \n6               Masha and Hero are getting divorced.  \n7               Masha and Hero are getting divorced.  \n8               Masha and Hero are getting divorced.  \n9  #Person1#: Happy birthday, Brian. #Person2#: I...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>instruct_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>Ms. Dawson will be out for an intra-office mem...</td>\n      <td>This memo is for employees only.</td>\n      <td>#Person1#: I need to take a dictation for you.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>Employees will be required to use instant mess...</td>\n      <td>#Person1#: Ms. Dawson, please type this memo i...</td>\n      <td>#Person1#: I need to take a dictation for you.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>The following memo is to be sent out to all em...</td>\n      <td>This is an intra-office memo.</td>\n      <td>#Person1#: I need to take a dictation for you.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>#Person1: You're here!</td>\n      <td>The conversation is about the future of public...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>People are talking about the traffic jams in t...</td>\n      <td>#Person1: I'm going to the office to work. #Pe...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>#P Person1#: I'm sorry to hear about your car ...</td>\n      <td>Taking public transport to work is a good option.</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>Masha and Hero are divorced.</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>People are talking about Masha and Hero.</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>#Person1#: Happy Birthday, Brian. #Person2#: Y...</td>\n      <td>#Person1#: Happy Birthday, Brian.</td>\n      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"raw","source":"Compute ROUGE score for this subset of the data. ","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:55:29.753117Z","iopub.execute_input":"2024-01-22T02:55:29.753421Z","iopub.status.idle":"2024-01-22T02:55:30.859267Z","shell.execute_reply.started":"2024-01-22T02:55:29.753395Z","shell.execute_reply":"2024-01-22T02:55:30.858068Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.21688642011010434, 'rouge2': 0.07751515151515151, 'rougeL': 0.18549546944283785, 'rougeLsum': 0.1884084141978879}\nINSTRUCT MODEL:\n{'rouge1': 0.28422212707926997, 'rouge2': 0.09696904830586307, 'rougeL': 0.2237588601874316, 'rougeLsum': 0.22708841423127135}\nPEFT MODEL:\n{'rouge1': 0.24089921652421653, 'rouge2': 0.11769053708439897, 'rougeL': 0.22001958689458687, 'rougeLsum': 0.22134175465057818}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice, that PEFT model results are not too bad, while the training process was much easier!","metadata":{}},{"cell_type":"markdown","source":"You already computed ROUGE score on the full dataset, after loading the results from the `data/dialogue-summary-training-results.csv` file. Load the values for the PEFT model now and check its performance compared to other models.","metadata":{}},{"cell_type":"code","source":"human_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\npeft_model_summaries     = results['peft_model_summaries'].values\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:55:30.860778Z","iopub.execute_input":"2024-01-22T02:55:30.861515Z","iopub.status.idle":"2024-01-22T02:55:45.420446Z","shell.execute_reply.started":"2024-01-22T02:55:30.861477Z","shell.execute_reply":"2024-01-22T02:55:45.419341Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2334158581572823, 'rouge2': 0.07603964187010573, 'rougeL': 0.20145520923859048, 'rougeLsum': 0.20145899339006135}\nINSTRUCT MODEL:\n{'rouge1': 0.42161291557556113, 'rouge2': 0.18035380596301792, 'rougeL': 0.3384439349963909, 'rougeLsum': 0.33835653595561666}\nPEFT MODEL:\n{'rouge1': 0.40810631575616746, 'rouge2': 0.1633255794568712, 'rougeL': 0.32507074586565354, 'rougeLsum': 0.3248950182867091}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The results show less of an improvement over full fine-tuning, but the benefits of PEFT typically outweigh the slightly-lower performance metrics.\n\nCalculate the improvement of PEFT over the original model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:55:45.421645Z","iopub.execute_input":"2024-01-22T02:55:45.421947Z","iopub.status.idle":"2024-01-22T02:55:45.428266Z","shell.execute_reply.started":"2024-01-22T02:55:45.421919Z","shell.execute_reply":"2024-01-22T02:55:45.427336Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\nrouge1: 17.47%\nrouge2: 8.73%\nrougeL: 12.36%\nrougeLsum: 12.34%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now calculate the improvement of PEFT over a full fine-tuned model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-22T02:55:45.429435Z","iopub.execute_input":"2024-01-22T02:55:45.429737Z","iopub.status.idle":"2024-01-22T02:55:45.444078Z","shell.execute_reply.started":"2024-01-22T02:55:45.429711Z","shell.execute_reply":"2024-01-22T02:55:45.443184Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\nrouge1: -1.35%\nrouge2: -1.70%\nrougeL: -1.34%\nrougeLsum: -1.35%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here you see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources (often just a single GPU).","metadata":{}},{"cell_type":"markdown","source":"Source:\n - https://www.coursera.org/learn/generative-ai-with-llms\n - https://www.coursera.org/learn/generative-ai-with-llms/gradedLti/x0gc1/lab-2-fine-tune-a-generative-ai-model-for-dialogue-summarization\n \nhttps://creativecommons.org/licenses/by-sa/2.0/legalcode","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}