{"cells":[{"cell_type":"markdown","metadata":{},"source":["# | NLP | PEFT/LoRA | DialogSum | Dialog Summarize |\n","\n","## NLP (Natural Language Processing) with PEFT (Parameter Efficient Fine-Tuning) and LoRA (Low-Rank Adaptation) for Dialogue Summarization\n","\n","# <b>1 <span style='color:#78D118'>|</span> Introduction</b>\n","\n","This project delves into the capabilities of LLM (Language Model) with a specific focus on leveraging Parameter Efficient Fine-Tuning (PEFT) for enhancing dialogue summarization using the FLAN-T5 model.\n","\n","Our goal is to enhance the quality of dialogue summarization by employing a comprehensive fine-tuning approach and evaluating the results using ROUGE metrics. Additionally, we will explore the advantages of Parameter Efficient Fine-Tuning (PEFT), demonstrating that its benefits outweigh any potential minor performance trade-offs.\n","\n"," - NOTE: This is an example and we not using the entirety of the data used for PERF / LoRA.\n"," \n","## Objectives :\n"," - Train LLM for Dialogue Summarization.\n"," \n"," \n"," ## The DialogSum Dataset:\n","The [DialogSum Dataset](https://huggingface.co/datasets/knkarthick/dialogsum) DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 (Plus 100 holdout data for topic generation) dialogues with corresponding manually labeled summaries and topics.\n","\n","## Project Workflow:\n","\n","- **Setup**: Import necessary libraries and define project parameters.\n","- **Dataset Exploration**: Discovering DialogSum Dataset.\n","- **Test Model Zero Shot Inferencing**: Initially, test the FLAN-T5 model for zero-shot inferencing on dialogue summarization tasks to establish a baseline performance.\n","- **Dataset Preprocess Dialog and Summary**: Preprocess the dialog and its corresponding summary from the dataset to prepare for the train.\n","-  **Perform Parameter Efficient Fine-Tuning (PEFT)**: Implement Parameter Efficient Fine-Tuning (PEFT), a more efficient fine-tuning approach that can significantly reduce training time while maintaining performance.\n","-  **Evaluation**:\n","    - Perform human evaluation to gauge the model's output in terms of readability and coherence. This can involve annotators ranking generated summaries for quality.\n","    - Utilize ROUGE metrics to assess the quality of the generated summaries. ROUGE measures the overlap between generated summaries and human-written references.\n","\n","# <b>2<span style='color:#78D118'>|</span> Setup</b>\n","## <b>2.1 <span style='color:#78D118'>|</span> Imports</b>"]},{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-01-26T14:33:49.196380Z","iopub.status.busy":"2024-01-26T14:33:49.195385Z","iopub.status.idle":"2024-01-26T14:33:49.201569Z","shell.execute_reply":"2024-01-26T14:33:49.200425Z","shell.execute_reply.started":"2024-01-26T14:33:49.196341Z"},"tags":[],"trusted":true},"outputs":[],"source":["# %pip install --upgrade pip\n","# %pip install --disable-pip-version-check \\\n","#     torch==1.13.1 \\\n","#     torchdata==0.5.1 --quiet\n","\n","# %pip install \\\n","#     transformers==4.27.2 \\\n","#     datasets==2.11.0 \\\n","#     evaluate==0.4.0 \\\n","#     rouge_score==0.1.2 \\\n","#     loralib==0.1.1 \\\n","#     peft==0.3.0 --quiet"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-26T14:33:49.204814Z","iopub.status.busy":"2024-01-26T14:33:49.204446Z","iopub.status.idle":"2024-01-26T14:33:49.213289Z","shell.execute_reply":"2024-01-26T14:33:49.212478Z","shell.execute_reply.started":"2024-01-26T14:33:49.204779Z"},"trusted":true},"outputs":[],"source":["# %pip install --upgrade pip\n","# %pip install \\\n","#     torch \\\n","#     torchdata --quiet\n","\n","# %pip install \\\n","#     transformers \\\n","#     datasets\\\n","#     evaluate \\\n","#     rouge_score \\\n","#     loralib \\\n","#     peft --quiet"]},{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:33:49.215196Z","iopub.status.busy":"2024-01-26T14:33:49.214870Z","iopub.status.idle":"2024-01-26T14:33:56.480983Z","shell.execute_reply":"2024-01-26T14:33:56.479983Z","shell.execute_reply.started":"2024-01-26T14:33:49.215169Z"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Job Applications and Interview - 2024\\MlOps-Assignment-2\\VENV_for_DialogSum_MLOps\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from datasets import load_dataset\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n","import torch\n","import time\n","import evaluate\n","import pandas as pd\n","import numpy as np\n","from peft import LoraConfig, get_peft_model, TaskType\n","from peft import PeftModel, PeftConfig"]},{"cell_type":"code","execution_count":2,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:33:56.483479Z","iopub.status.busy":"2024-01-26T14:33:56.482697Z","iopub.status.idle":"2024-01-26T14:33:57.004477Z","shell.execute_reply":"2024-01-26T14:33:57.003494Z","shell.execute_reply.started":"2024-01-26T14:33:56.483448Z"},"tags":[],"trusted":true},"outputs":[],"source":["rouge = evaluate.load('rouge')\n","dash_line = '-'.join('' for x in range(100))"]},{"cell_type":"markdown","metadata":{},"source":["Load the dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-26T14:33:57.005957Z","iopub.status.busy":"2024-01-26T14:33:57.005663Z","iopub.status.idle":"2024-01-26T14:33:57.010903Z","shell.execute_reply":"2024-01-26T14:33:57.009896Z","shell.execute_reply.started":"2024-01-26T14:33:57.005931Z"},"trusted":true},"outputs":[],"source":["# %pip install -U datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-01-26T14:33:57.013895Z","iopub.status.busy":"2024-01-26T14:33:57.013571Z","iopub.status.idle":"2024-01-26T14:33:59.150344Z","shell.execute_reply":"2024-01-26T14:33:59.149409Z","shell.execute_reply.started":"2024-01-26T14:33:57.013869Z"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset csv (C:/Users/Prajwal-S-Yallur/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-3005b557c2c04c1d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n","100%|██████████| 3/3 [00:00<00:00, 38.83it/s]\n"]}],"source":["huggingface_dataset_name = \"knkarthick/dialogsum\"\n","dataset = load_dataset(huggingface_dataset_name)"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["Load the pre-trained [FLAN-T5 model](https://huggingface.co/google/flan-t5-base) and its tokenizer directly from Hugging Face. We'll be using the smaller version of FLAN-T5 for this project.\n","\n","To optimize memory usage, set `torch_dtype=torch.bfloat16` to specify the memory type used by this model."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-26T14:33:59.151667Z","iopub.status.busy":"2024-01-26T14:33:59.151382Z","iopub.status.idle":"2024-01-26T14:33:59.156108Z","shell.execute_reply":"2024-01-26T14:33:59.155093Z","shell.execute_reply.started":"2024-01-26T14:33:59.151641Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":10,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-01-26T14:33:59.158123Z","iopub.status.busy":"2024-01-26T14:33:59.157571Z","iopub.status.idle":"2024-01-26T14:34:07.565066Z","shell.execute_reply":"2024-01-26T14:34:07.564007Z","shell.execute_reply.started":"2024-01-26T14:33:59.158088Z"},"tags":[],"trusted":true},"outputs":[],"source":["model_name='google/flan-t5-base'\n","original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"markdown","metadata":{},"source":["## <b>2.2 <span style='color:#78D118'>|</span> Methods</b>"]},{"cell_type":"code","execution_count":11,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-01-26T14:34:07.566652Z","iopub.status.busy":"2024-01-26T14:34:07.566343Z","iopub.status.idle":"2024-01-26T14:34:07.574519Z","shell.execute_reply":"2024-01-26T14:34:07.573115Z","shell.execute_reply.started":"2024-01-26T14:34:07.566626Z"},"trusted":true},"outputs":[],"source":["def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n","\n","def tokenize_function(example):\n","    start_prompt = 'Summarize the following conversation.\\n\\n'\n","    end_prompt = '\\n\\nSummary: '\n","    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n","    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n","    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n","    \n","    return example"]},{"cell_type":"markdown","metadata":{},"source":["# <b>3<span style='color:#78D118'>|</span> Data Exploration</b>"]},{"cell_type":"code","execution_count":12,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:34:07.576383Z","iopub.status.busy":"2024-01-26T14:34:07.576088Z","iopub.status.idle":"2024-01-26T14:34:07.598312Z","shell.execute_reply":"2024-01-26T14:34:07.596597Z","shell.execute_reply.started":"2024-01-26T14:34:07.576357Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","trainable model parameters: 247577856\n","all model parameters: 247577856\n","percentage of trainable model parameters: 100.00%\n","---------------------------------------------------------------------------------------------------\n"]}],"source":["print(dash_line)\n","print(print_number_of_trainable_model_parameters(original_model))\n","print(dash_line)"]},{"cell_type":"code","execution_count":13,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:34:07.600429Z","iopub.status.busy":"2024-01-26T14:34:07.599938Z","iopub.status.idle":"2024-01-26T14:34:07.611002Z","shell.execute_reply":"2024-01-26T14:34:07.610121Z","shell.execute_reply.started":"2024-01-26T14:34:07.600371Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","---------------------------------------------------------------------------------------------------\n","\n","PROMPT:\n","\n","Summarize the following conversation.\n","\n","\n","#Person1#: Have you considered upgrading your system?\n","\n","#Person2#: Yes, but I'm not sure what exactly I would need.\n","\n","#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n","\n","#Person2#: That would be a definite bonus.\n","\n","#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n","\n","#Person2#: How can we do that?\n","\n","#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n","\n","#Person2#: No.\n","\n","#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n","\n","#Person2#: That sounds great. Thanks.\n","\n","\n","Summary:\n","\n","---------------------------------------------------------------------------------------------------\n","\n","HUMAN SUMMARY:\n","\n","#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n","\n","---------------------------------------------------------------------------------------------------\n","    \n"]}],"source":["print(\n","    \"\"\"\n","---------------------------------------------------------------------------------------------------\n","\n","PROMPT:\n","\n","Summarize the following conversation.\n","\n","\n","#Person1#: Have you considered upgrading your system?\n","\n","#Person2#: Yes, but I'm not sure what exactly I would need.\n","\n","#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n","\n","#Person2#: That would be a definite bonus.\n","\n","#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n","\n","#Person2#: How can we do that?\n","\n","#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n","\n","#Person2#: No.\n","\n","#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n","\n","#Person2#: That sounds great. Thanks.\n","\n","\n","Summary:\n","\n","---------------------------------------------------------------------------------------------------\n","\n","HUMAN SUMMARY:\n","\n","#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n","\n","---------------------------------------------------------------------------------------------------\n","    \"\"\"\n",")"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["# <b>4<span style='color:#78D118'>|</span> Test Model Zero Shot Inferencing</b>\n","\n","Test the model using zero-shot inference. It's evident that the model faces challenges in summarizing the dialogue when compared to the baseline summary. However, it manages to extract some crucial information from the text, suggesting that fine-tuning."]},{"cell_type":"code","execution_count":14,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:34:46.852429Z","iopub.status.busy":"2024-01-26T14:34:46.851998Z","iopub.status.idle":"2024-01-26T14:34:48.922199Z","shell.execute_reply":"2024-01-26T14:34:48.921176Z","shell.execute_reply.started":"2024-01-26T14:34:46.852396Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","ZERO SHOT\n","---------------------------------------------------------------------------------------------------\n","PROMPT:\n","\n","Summarize the following conversation.\n","\n","#Person1#: Have you considered upgrading your system?\n","#Person2#: Yes, but I'm not sure what exactly I would need.\n","#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n","#Person2#: That would be a definite bonus.\n","#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n","#Person2#: How can we do that?\n","#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n","#Person2#: No.\n","#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n","#Person2#: That sounds great. Thanks.\n","\n","Summary:\n","\n","---------------------------------------------------------------------------------------------------\n","HUMAN SUMMARY:\n","#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n","\n","---------------------------------------------------------------------------------------------------\n","ORIGINAL MODEL SUMMARY:\n","#Person1#: I'm thinking of upgrading my computer.\n","---------------------------------------------------------------------------------------------------\n"]}],"source":["index = 200\n","\n","dialogue = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","{dialogue}\n","\n","Summary:\n","\"\"\"\n","\n","inputs = tokenizer(prompt, return_tensors='pt').to(device)\n","output = tokenizer.decode(\n","    original_model.generate(\n","        inputs[\"input_ids\"], \n","        max_new_tokens=200,\n","    )[0], \n","    skip_special_tokens=True\n",")\n","print(dash_line)\n","print(\"ZERO SHOT\")\n","print(dash_line)\n","print(f'PROMPT:\\n{prompt}')\n","print(dash_line)\n","print(f'HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'ORIGINAL MODEL SUMMARY:\\n{output}')\n","print(dash_line)"]},{"cell_type":"markdown","metadata":{},"source":["# <b>5<span style='color:#78D118'>|</span> Dataset Preprocess Dialog and Summary</b>"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["Transform the dialog-summary (prompt-response) pairs by adding specific instructions for the Language Model (LLM). Add the instruction \"Summarize the following conversation\" at the beginning of the dialog and \"Summary\" at the beginning of the summary like this:\n","\n","Training prompt (dialogue):\n","```\n","Summarize the following conversation.\n","\n","    Chris: This is his part of the conversation.\n","    Antje: This is her part of the conversation.\n","    \n","Summary: \n","```\n","\n","Training response (summary):\n","```\n","Both Chris and Antje participated in the conversation.\n","```\n","\n","Now we preprocess the prompt-response dataset by tokenizing the text and extracting their input_ids, with one input_id assigned per token."]},{"cell_type":"code","execution_count":16,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-01-26T14:36:15.300678Z","iopub.status.busy":"2024-01-26T14:36:15.300269Z","iopub.status.idle":"2024-01-26T14:36:28.656061Z","shell.execute_reply":"2024-01-26T14:36:28.655258Z","shell.execute_reply.started":"2024-01-26T14:36:15.300646Z"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"830b82670f2241e28d3577ff1aaf6332","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"]},{"cell_type":"code","execution_count":15,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-01-26T14:35:29.804835Z","iopub.status.busy":"2024-01-26T14:35:29.804460Z","iopub.status.idle":"2024-01-26T14:35:39.627668Z","shell.execute_reply":"2024-01-26T14:35:39.626713Z","shell.execute_reply.started":"2024-01-26T14:35:29.804808Z"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9457edcb076143abbda5059824cf8769","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12fb2e46234546e69f4e0177a5fdb331","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f21e13d17e247ff96d7a3ae6bdce6b7","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 10 == 0, with_indices=True)"]},{"cell_type":"markdown","metadata":{},"source":[" - NOTE: This is an example and we not using the entirety of the data used for PERF / LoRA."]},{"cell_type":"code","execution_count":17,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:36:35.022132Z","iopub.status.busy":"2024-01-26T14:36:35.021145Z","iopub.status.idle":"2024-01-26T14:36:35.027851Z","shell.execute_reply":"2024-01-26T14:36:35.026810Z","shell.execute_reply.started":"2024-01-26T14:36:35.022100Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","Shapes of the datasets:\n","Training: (12460, 2)\n","Validation: (500, 2)\n","Test: (1500, 2)\n","DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'labels'],\n","        num_rows: 12460\n","    })\n","    validation: Dataset({\n","        features: ['input_ids', 'labels'],\n","        num_rows: 500\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'labels'],\n","        num_rows: 1500\n","    })\n","})\n","---------------------------------------------------------------------------------------------------\n"]}],"source":["print(dash_line)\n","print(f\"Shapes of the datasets:\")\n","print(f\"Training: {tokenized_datasets['train'].shape}\")\n","print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n","print(f\"Test: {tokenized_datasets['test'].shape}\")\n","print(tokenized_datasets)\n","print(dash_line)"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["Check the shapes of all three parts of the dataset:"]},{"cell_type":"markdown","metadata":{},"source":["# <b>6 <span style='color:#78D118'>|</span> Dataset Preprocess Dialog and Summary</b>\n","\n","Let's delve into the process of Parameter Efficient Fine-Tuning (PEFT), which offers a more efficient alternative to full fine-tuning. PEFT encompasses various techniques, including Low-Rank Adaptation (LoRA) and prompt tuning (distinct from prompt engineering).\n","\n","PEFT, it typically involves Low-Rank Adaptation (LoRA).\n","\n","LoRA, in essence, enables fine-tuning of your model with significantly fewer computational resources, sometimes even just a single GPU. After fine-tuning for a specific task, use case, or tenant using LoRA, the original Language Model (LLM) remains unchanged, while a newly-trained \"LoRA adapter\" emerges. This LoRA adapter is substantially smaller than the original LLM, often only a fraction of its size (in megabytes rather than gigabytes).\n","\n","However, during inference, the LoRA adapter needs to be reintegrated and combined with its original LLM to fulfill the inference request. The advantage lies in the fact that multiple LoRA adapters can reuse the same original LLM, reducing overall memory requirements when serving multiple tasks and use cases."]},{"cell_type":"markdown","metadata":{},"source":["## <b>6.1 <span style='color:#78D118'>|</span> PEFT/LoRA model for Fine-Tuning</b>\n","\n","To configure the PEFT/LoRA model for fine-tuning with a new parameter adapter, we follow these steps:\n","\n","1. **PEFT/LoRA Setup**: \n","   - We are using PEFT/LoRA, which means we freeze the underlying Language Model (LLM) and focus on training only the adapter.\n","\n","2. **Adapter Configuration**:\n","   - LoRA configuration below, the `rank (r)` hyper-parameter. This hyper-parameter determines the rank or dimensionality of the adapter that will be trained.\n","\n","By employing PEFT/LoRA, we ensure that the core LLM remains unchanged while adapting a separate parameterized layer for our specific task or use case. The `rank (r)` hyper-parameter plays a critical role in determining the adapter's complexity and capacity for the target task."]},{"cell_type":"code","execution_count":20,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:37:25.456386Z","iopub.status.busy":"2024-01-26T14:37:25.455562Z","iopub.status.idle":"2024-01-26T14:37:25.461914Z","shell.execute_reply":"2024-01-26T14:37:25.460775Z","shell.execute_reply.started":"2024-01-26T14:37:25.456347Z"},"tags":[],"trusted":true},"outputs":[],"source":["lora_config = LoraConfig(\n","    r=64, # Rank\n","    lora_alpha=32,\n","    target_modules=[\"q\", \"v\"],\n","    lora_dropout=0.15,\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",")"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["Incorporate LoRA adapter layers and parameters into the original Language Model (LLM) for training."]},{"cell_type":"code","execution_count":21,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:37:30.165172Z","iopub.status.busy":"2024-01-26T14:37:30.164200Z","iopub.status.idle":"2024-01-26T14:37:30.344831Z","shell.execute_reply":"2024-01-26T14:37:30.343860Z","shell.execute_reply.started":"2024-01-26T14:37:30.165136Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable model parameters: 7077888\n","all model parameters: 254655744\n","percentage of trainable model parameters: 2.78%\n"]}],"source":["peft_model = get_peft_model(original_model, \n","                            lora_config)\n","print(print_number_of_trainable_model_parameters(peft_model))"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## <b>6.2 <span style='color:#78D118'>|</span> Train PEFT/LoRA Adapter</b>"]},{"cell_type":"code","execution_count":26,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:40:43.371994Z","iopub.status.busy":"2024-01-26T14:40:43.371595Z","iopub.status.idle":"2024-01-26T14:40:43.396393Z","shell.execute_reply":"2024-01-26T14:40:43.395201Z","shell.execute_reply.started":"2024-01-26T14:40:43.371966Z"},"tags":[],"trusted":true},"outputs":[],"source":["output_dir = f'./peft-dialogue-summary-training'\n","\n","peft_training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    auto_find_batch_size=True,\n","    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n","    num_train_epochs=1,\n","    logging_steps=1,\n","    max_steps=100,\n","    do_eval=True\n",")\n","    \n","peft_trainer = Trainer(\n","    model=peft_model,\n","    args=peft_training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"]\n",")"]},{"cell_type":"code","execution_count":27,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-01-26T14:40:47.457775Z","iopub.status.busy":"2024-01-26T14:40:47.457023Z","iopub.status.idle":"2024-01-26T14:42:57.497276Z","shell.execute_reply":"2024-01-26T14:42:57.495974Z","shell.execute_reply.started":"2024-01-26T14:40:47.457741Z"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 02:07, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>26.375000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>23.250000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>19.500000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>16.625000</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>12.250000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>8.312500</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>5.968800</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>4.937500</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>4.718800</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>4.500000</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>4.468800</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>4.406200</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>4.312500</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>4.250000</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>4.187500</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>4.187500</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>4.031200</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>4.031200</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>3.921900</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>3.812500</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>3.765600</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>3.640600</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>3.421900</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>3.265600</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>3.171900</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>3.031200</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>2.875000</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>2.671900</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>2.609400</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>2.484400</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>2.250000</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>2.125000</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>1.953100</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>1.890600</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>2.171900</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>1.703100</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>1.695300</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>1.484400</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>1.429700</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.367200</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>1.328100</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>1.265600</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>1.335900</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>1.125000</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>1.234400</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.988300</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.976600</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.859400</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.843800</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.793000</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.796900</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.738300</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.793000</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.707000</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.656200</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.738300</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.941400</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.664100</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.601600</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.621100</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.609400</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.546900</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.550800</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.539100</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.519500</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.687500</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.515600</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.503900</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.742200</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.466800</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.482400</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.468800</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.554700</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.427700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.543000</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.447300</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.445300</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.418000</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.378900</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.410200</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.443400</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.419900</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.466800</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.390600</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.480500</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.404300</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.429700</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.423800</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.535200</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.390600</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.427700</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.392600</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.369100</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.441400</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.416000</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.535200</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.453100</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.439500</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.425800</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.527300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model Training time is: 130031.78906440735 ms\n"]}],"source":["# record start time\n","start = time.time()\n","\n","peft_trainer.train()\n","\n","# record end time\n","end = time.time()\n","\n","\n","# print the difference between start \n","# and end time in milli. secs\n","print(\"Model Training time is:\",\n","      (end-start), \"s\")"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-01-26T14:46:13.743064Z","iopub.status.busy":"2024-01-26T14:46:13.742596Z","iopub.status.idle":"2024-01-26T14:46:13.820542Z","shell.execute_reply":"2024-01-26T14:46:13.819456Z","shell.execute_reply.started":"2024-01-26T14:46:13.743020Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n"," './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n"," './peft-dialogue-summary-checkpoint-local/spiece.model',\n"," './peft-dialogue-summary-checkpoint-local/added_tokens.json',\n"," './peft-dialogue-summary-checkpoint-local/tokenizer.json')"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n","\n","peft_trainer.model.save_pretrained(peft_model_path)\n","tokenizer.save_pretrained(peft_model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:34:10.047415Z","iopub.status.idle":"2024-01-26T14:34:10.047760Z","shell.execute_reply":"2024-01-26T14:34:10.047609Z","shell.execute_reply.started":"2024-01-26T14:34:10.047592Z"},"trusted":true},"outputs":[],"source":["peft_model_path=\"./peft-dialogue-summary-checkpoint-local\""]},{"cell_type":"code","execution_count":29,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:46:22.245412Z","iopub.status.busy":"2024-01-26T14:46:22.245002Z","iopub.status.idle":"2024-01-26T14:46:25.108601Z","shell.execute_reply":"2024-01-26T14:46:25.107341Z","shell.execute_reply.started":"2024-01-26T14:46:22.245378Z"},"tags":[],"trusted":true},"outputs":[],"source":["peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n","\n","peft_model = PeftModel.from_pretrained(peft_model_base, \n","                                       peft_model_path, \n","                                       torch_dtype=torch.bfloat16,\n","                                       is_trainable=False)"]},{"cell_type":"code","execution_count":30,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:46:30.146594Z","iopub.status.busy":"2024-01-26T14:46:30.145679Z","iopub.status.idle":"2024-01-26T14:46:30.161163Z","shell.execute_reply":"2024-01-26T14:46:30.160015Z","shell.execute_reply.started":"2024-01-26T14:46:30.146555Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable model parameters: 0\n","all model parameters: 254655744\n","percentage of trainable model parameters: 0.00%\n"]}],"source":["print(print_number_of_trainable_model_parameters(peft_model))"]},{"cell_type":"markdown","metadata":{},"source":["# <b>7 <span style='color:#78D118'>|</span> Evaluation</b>\n","\n","## <b>7.1 <span style='color:#78D118'>|</span> Evaluate the Model Qualitatively (Human Evaluation)</b>"]},{"cell_type":"code","execution_count":31,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:46:35.660997Z","iopub.status.busy":"2024-01-26T14:46:35.660093Z","iopub.status.idle":"2024-01-26T14:46:37.982113Z","shell.execute_reply":"2024-01-26T14:46:37.980861Z","shell.execute_reply.started":"2024-01-26T14:46:35.660964Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","HUMAN SUMMARY:\n","#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n","---------------------------------------------------------------------------------------------------\n","ORIGINAL MODEL:\n","#Person1## wants to upgrade his hardware. #Person2## needs a faster processor, to begin with.\n","---------------------------------------------------------------------------------------------------\n","PEFT MODEL: #Person1# wants to upgrade his hardware because it is pretty outdated now. #Person1# wants to upgrade his hardware because it is pretty outdated now. #Person1# wants to upgrade his computer. #Person1# wants to add a painting program to his software.\n","---------------------------------------------------------------------------------------------------\n"]}],"source":["index = 200\n","dialogue = dataset['test'][index]['dialogue']\n","human_baseline_summaries = dataset['test'][index]['summary']\n","\n","prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","{dialogue}\n","\n","Summary: \"\"\"\n","\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device).input_ids\n","\n","original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n","original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n","\n","peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n","peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n","\n","print(dash_line)\n","print(f'HUMAN SUMMARY:\\n{human_baseline_summaries}')\n","print(dash_line)\n","print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n","print(dash_line)\n","print(f'PEFT MODEL: {peft_model_text_output}')\n","print(dash_line)"]},{"cell_type":"code","execution_count":32,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T14:47:03.879580Z","iopub.status.busy":"2024-01-26T14:47:03.878744Z","iopub.status.idle":"2024-01-26T14:48:09.337105Z","shell.execute_reply":"2024-01-26T14:48:09.336007Z","shell.execute_reply.started":"2024-01-26T14:47:03.879545Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Inference time is: 65.43029761314392 s\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>human_baseline_summaries</th>\n","      <th>original_model_summaries</th>\n","      <th>peft_model_summaries</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n","      <td>Memo1### is a #Person1## dicting for #Person1#...</td>\n","      <td>#Person1# needs to take a dictation for #Perso...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>In order to prevent employees from wasting tim...</td>\n","      <td>#Person1## needs to take a dictation to #Perso...</td>\n","      <td>#Person1# needs to take a dictation for #Perso...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n","      <td>#Person1# needs to take a dictation for #Perso...</td>\n","      <td>#Person1# needs to take a dictation for #Perso...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>#Person2# arrives late because of traffic jam....</td>\n","      <td>#Person1# feels bad about his car's congestion...</td>\n","      <td>#Person1# is going to have to consider a diffe...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n","      <td>You're finally here, #Person1#: #Person1# thin...</td>\n","      <td>#Person1# is going to have to consider a diffe...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>#Person2# complains to #Person1# about the tra...</td>\n","      <td>#Person1# is stuck in traffic jam near the Car...</td>\n","      <td>#Person1# is going to have to consider a diffe...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n","      <td>#Person1##: Masha and Hero are getting divorce...</td>\n","      <td>#Person1# is having a separation for 2 months,...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n","      <td>#Person1#: #Person1##, Masha and Hero are havi...</td>\n","      <td>#Person1# is having a separation for 2 months,...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>#Person1# and Kate talk about the divorce betw...</td>\n","      <td>Masha and Hero are having a separation for 2 m...</td>\n","      <td>#Person1# is having a separation for 2 months,...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>#Person1# and Brian are at the birthday party ...</td>\n","      <td>You are always popular with everyone, and you ...</td>\n","      <td>#Person1# is always popular with everyone. #Pe...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            human_baseline_summaries  \\\n","0  Ms. Dawson helps #Person1# to write a memo to ...   \n","1  In order to prevent employees from wasting tim...   \n","2  Ms. Dawson takes a dictation for #Person1# abo...   \n","3  #Person2# arrives late because of traffic jam....   \n","4  #Person2# decides to follow #Person1#'s sugges...   \n","5  #Person2# complains to #Person1# about the tra...   \n","6  #Person1# tells Kate that Masha and Hero get d...   \n","7  #Person1# tells Kate that Masha and Hero are g...   \n","8  #Person1# and Kate talk about the divorce betw...   \n","9  #Person1# and Brian are at the birthday party ...   \n","\n","                            original_model_summaries  \\\n","0  Memo1### is a #Person1## dicting for #Person1#...   \n","1  #Person1## needs to take a dictation to #Perso...   \n","2  #Person1# needs to take a dictation for #Perso...   \n","3  #Person1# feels bad about his car's congestion...   \n","4  You're finally here, #Person1#: #Person1# thin...   \n","5  #Person1# is stuck in traffic jam near the Car...   \n","6  #Person1##: Masha and Hero are getting divorce...   \n","7  #Person1#: #Person1##, Masha and Hero are havi...   \n","8  Masha and Hero are having a separation for 2 m...   \n","9  You are always popular with everyone, and you ...   \n","\n","                                peft_model_summaries  \n","0  #Person1# needs to take a dictation for #Perso...  \n","1  #Person1# needs to take a dictation for #Perso...  \n","2  #Person1# needs to take a dictation for #Perso...  \n","3  #Person1# is going to have to consider a diffe...  \n","4  #Person1# is going to have to consider a diffe...  \n","5  #Person1# is going to have to consider a diffe...  \n","6  #Person1# is having a separation for 2 months,...  \n","7  #Person1# is having a separation for 2 months,...  \n","8  #Person1# is having a separation for 2 months,...  \n","9  #Person1# is always popular with everyone. #Pe...  "]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["dialogues = dataset['test'][0:10]['dialogue']\n","human_baseline_summaries = dataset['test'][0:10]['summary']\n","\n","original_model_summaries = []\n","instruct_model_summaries = []\n","peft_model_summaries = []\n","\n","\n","# record start time\n","start = time.time()\n","\n","for idx, dialogue in enumerate(dialogues):\n","    prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","{dialogue}\n","\n","Summary: \"\"\"\n","    \n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device).input_ids\n","\n","    human_baseline_text_output = human_baseline_summaries[idx]\n","    \n","    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n","    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n","\n","    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n","    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n","\n","    original_model_summaries.append(original_model_text_output)\n","    peft_model_summaries.append(peft_model_text_output)\n","    \n","#     print(dash_line)\n","#     print(f'HUMAN SUMMARY:\\n{human_baseline_summaries}')\n","#     print(dash_line)\n","#     print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n","#     print(dash_line)\n","#     print(f'PEFT MODEL: {peft_model_text_output}')\n","#     print(dash_line)\n","#     print(dash_line)\n","\n","# record end time\n","end = time.time()\n","\n","\n","# print the difference between start \n","# and end time in milli. secs\n","print(\"Model Inference time is:\",\n","      (end-start), \"s\")\n","\n","zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n"," \n","df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:34:10.054683Z","iopub.status.idle":"2024-01-26T14:34:10.055004Z","shell.execute_reply":"2024-01-26T14:34:10.054851Z","shell.execute_reply.started":"2024-01-26T14:34:10.054837Z"},"trusted":true},"outputs":[],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-26T14:50:27.284042Z","iopub.status.busy":"2024-01-26T14:50:27.282868Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\n","101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200\n","201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, "]},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 512). Running this sequence through the model will result in indexing errors\n"]},{"name":"stdout","output_type":"stream","text":["260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300\n","301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400\n","401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500\n","501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600\n","601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700\n","701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800\n","801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900\n","901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000\n","1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100\n","1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, "]}],"source":["dialogues = dataset['test'][:]['dialogue']\n","human_baseline_summaries = dataset['test'][:]['summary']\n","\n","original_model_summaries = []\n","instruct_model_summaries = []\n","peft_model_summaries = []\n","\n","\n","# record start time\n","start = time.time()\n","\n","\n","for idx, dialogue in enumerate(dialogues):\n","    prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","{dialogue}\n","\n","Summary: \"\"\"\n","    \n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device).input_ids\n","\n","    human_baseline_text_output = human_baseline_summaries[idx]\n","    \n","    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n","    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n","\n","    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n","    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n","\n","    original_model_summaries.append(original_model_text_output)\n","    peft_model_summaries.append(peft_model_text_output)\n","    \n","#     print(dash_line)\n","#     print(f'HUMAN SUMMARY:\\n{human_baseline_summaries}')\n","#     print(dash_line)\n","#     print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n","#     print(dash_line)\n","#     print(f'PEFT MODEL: {peft_model_text_output}')\n","#     print(dash_line)\n","#     print(dash_line)\n","    if idx % 100 == 0:\n","        print(idx)\n","    else:\n","        print(idx, end=\", \")\n","        \n","# record end time\n","end = time.time()\n","\n","# print the difference between start \n","# and end time in milli. secs\n","print(\"Model Inference time is:\",\n","      (end-start), \"s\")\n","\n","zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n"," \n","df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n","df"]},{"cell_type":"markdown","metadata":{},"source":["## <b>7.2 <span style='color:#78D118'>|</span> Evaluate the Model Quantitatively (ROUGE Metric)</b>\n","\n","The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) is a valuable tool for assessing the quality of summaries generated by models. It evaluates these summaries by comparing them to a \"baseline\" summary, typically crafted by a human. Although not flawless, the ROUGE metric provides insights into the improvement in the overall effectiveness of summarization achieved through fine-tuning."]},{"cell_type":"code","execution_count":37,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T16:40:02.270457Z","iopub.status.busy":"2024-01-26T16:40:02.270124Z","iopub.status.idle":"2024-01-26T16:40:16.212586Z","shell.execute_reply":"2024-01-26T16:40:16.211085Z","shell.execute_reply.started":"2024-01-26T16:40:02.270427Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","ORIGINAL MODEL:\n","{'rouge1': 0.22736904083260462, 'rouge2': 0.05697901366251684, 'rougeL': 0.18679210189076217, 'rougeLsum': 0.18687460055156696}\n","---------------------------------------------------------------------------------------------------\n","PEFT MODEL:\n","{'rouge1': 0.19934745161507816, 'rouge2': 0.057275918136077124, 'rougeL': 0.17116570635572304, 'rougeLsum': 0.17125606161128704}\n","---------------------------------------------------------------------------------------------------\n"]}],"source":["human_baseline_summaries = df['human_baseline_summaries'].values\n","original_model_summaries = df['original_model_summaries'].values\n","peft_model_summaries     = df['peft_model_summaries'].values\n","\n","original_model_results = rouge.compute(\n","    predictions=original_model_summaries,\n","    references=human_baseline_summaries[0:len(original_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","peft_model_results = rouge.compute(\n","    predictions=peft_model_summaries,\n","    references=human_baseline_summaries[0:len(peft_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","print(dash_line)\n","print('ORIGINAL MODEL:')\n","print(original_model_results)\n","print(dash_line)\n","print('PEFT MODEL:')\n","print(peft_model_results)\n","print(dash_line)"]},{"cell_type":"code","execution_count":38,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-01-26T16:55:54.586813Z","iopub.status.busy":"2024-01-26T16:55:54.586342Z","iopub.status.idle":"2024-01-26T16:55:54.599264Z","shell.execute_reply":"2024-01-26T16:55:54.597587Z","shell.execute_reply.started":"2024-01-26T16:55:54.586781Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n","rouge1: -2.80%\n","rouge2: 0.03%\n","rougeL: -1.56%\n","rougeLsum: -1.56%\n"]}],"source":["print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n","\n","improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n","for key, value in zip(peft_model_results.keys(), improvement):\n","    print(f'{key}: {value*100:.2f}%')"]},{"cell_type":"markdown","metadata":{},"source":["## References\n","\n","The creation of this document was greatly influenced by the following key sources of information:\n","\n","1. [DialogSum Dataset](https://huggingface.co/datasets/knkarthick/dialogsum) DialogSum is a large-scale dialogue summarization dataset, consisting of 13,460 (Plus 100 holdout data for topic generation) dialogues with corresponding manually labeled summaries and topics.\n","2. [Generative AI with Large Language Models | Coursera](https://www.coursera.org/learn/generative-ai-with-llms?utm_medium=sem&utm_source=gg&utm_campaign=B2C_NAMER_generative-ai-with-llms_deeplearning-ai_FTCOF_learn_country-US-country-CA&campaignid=20534248984&adgroupid=160068579824&device=c&keyword=&matchtype=&network=g&devicemodel=&adposition=&creativeid=673251286004&hide_mobile_promo&gclid=CjwKCAjwg4SpBhAKEiwAdyLwvEW_WnNyptOwzHtsGmn5-OxT5BKsQeUXHPahO-opBJ0JjsSynHkPAxoCaoAQAvD_BwE) - An informative guide that provides in-depth explanations and examples on various LLMs."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3659633,"sourceId":6354265,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
